{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this assignment, you are going to build a classifier for named entities from the Groningen Meaning Bank corpus.  Named entity recognition (NER) takes noun phrases from a text and identifies whether they are persons, organizations, and so on.  You will be using the Groningen Meaning Bank named entity corpus available on mltgpu at `/scratch/lt2222-v21-resources/GMB_dataset.txt`.  In this version of the task, you will assume we know *that* something is a named entity, and instead use multi-class classification to identify its type.  So you will be doing named entity classification but *not* recognition.\n",
    "\n",
    "The data looks like this: \n",
    "\n",
    "```\n",
    "3996    182.0   Nicole  NNP     B-per\n",
    "3997    182.0   Ritchie NNP     I-per\n",
    "3998    182.0   is      VBZ     O\n",
    "3999    182.0   pregnant        JJ      O\n",
    "4000    182.0   .       .       O\n",
    "4001    183.0   Speaking        VBG     O\n",
    "4002    183.0   to      TO      O\n",
    "4003    183.0   ABC     NNP     B-org\n",
    "4004    183.0   News    NNP     I-org\n",
    "4005    183.0   interviewer     NN      O\n",
    "4006    183.0   Dianne  NNP     B-per\n",
    "4007    183.0   Sawyer  NNP     I-per\n",
    "4008    183.0   ,       ,       O\n",
    "4009    183.0   the     DT      O\n",
    "4010    183.0   25-year-old     JJ      O\n",
    "4011    183.0   co-star NN      O\n",
    "4012    183.0   of      IN      O\n",
    "4013    183.0   TV      NN      O\n",
    "4014    183.0   's      POS     O\n",
    "4015    183.0   The     DT      B-art\n",
    "4016    183.0   Simple  NNP     I-art\n",
    "4017    183.0   Life    NNP     I-art\n",
    "4018    183.0   said    VBD     O\n",
    "4019    183.0   she     PRP     O\n",
    "4020    183.0   is      VBZ     O\n",
    "4021    183.0   almost  RB      O\n",
    "4022    183.0   four    CD      O\n",
    "4023    183.0   months  NNS     O\n",
    "4024    183.0   along   IN      O\n",
    "4025    183.0   in      IN      O\n",
    "4026    183.0   her     PRP$    O\n",
    "4027    183.0   pregnancy       NN      O\n",
    "4028    183.0   .       .       O\n",
    "```\n",
    "\n",
    "The first column is the line number.  The second column is a sentence number (for some reason given as a float; ignore it).  The third column is the word.  The fourth column is a part of speech (POS) tag in Penn Treebank format.  The last column contains the named entity annotation. \n",
    "\n",
    "The annotation works like this.  Every `O` just means that the row does not represent a named entity.  `B-xyx` means the first word in a named entity with type `xyx`. `I-xyz` means the second and later words of an `xyz` entity, if there are any.  That means that every time there's a `B` or an `I`, there's a named entity.  \n",
    "\n",
    "The entity types in the corpus are `art`,\n",
    "`eve`,\n",
    "`geo`,\n",
    "`gpe`,\n",
    "`nat`,\n",
    "`org`,\n",
    "`per`,\n",
    "and `tim`\n",
    "\n",
    "Your task is the following.\n",
    "1. To preprocess the text (lowercase and lemmatize; punctuation can be preserved as it gets its own rows).\n",
    "2. To create instances from every from every identified named entity in the text with the type of the NE as the class, and a surrounding context of five words on either side as the features.  \n",
    "3. To generate vectors and split the instances into training and testing datasets at random.\n",
    "4. To train a support vector machine (via `sklearn.svm.LinearSVC`) for classifying the NERs.\n",
    "5. To evaluate the performance of the classifier.\n",
    "\n",
    "You will do this by modifying a separate file containing functions that will be called from this notebook as a module.  You can modify this notebook for testing purposes but please only submit the original.  You will document everything in Markdown in README.md and submit a GitHub repository URL.\n",
    "\n",
    "This assignment is due on **Tuesday, 2021 March 9 at 23:59**.  It has **25 points** and **7 bonus points**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import a2\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmbfile = open('/scratch/lt2222-v21-resources/GMB_dataset.txt', \"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - preprocessing (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See step 1 above.  The data is coming to you as an unused file handle object.  You can return the data in any indexable form you like.  You can also choose to remove infrequent or uninformative words to reduce the size of the feature space. (Document this in README.md.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 'thousand', 'NNS', 'O'],\n",
       " [1, 1, 'of', 'IN', 'O'],\n",
       " [2, 1, 'demonstrator', 'NNS', 'O'],\n",
       " [3, 1, 'have', 'VBP', 'O'],\n",
       " [4, 1, 'marched', 'VBN', 'O'],\n",
       " [5, 1, 'through', 'IN', 'O'],\n",
       " [6, 1, 'london', 'NNP', 'B-geo'],\n",
       " [7, 1, 'to', 'TO', 'O'],\n",
       " [8, 1, 'protest', 'VB', 'O'],\n",
       " [9, 1, 'the', 'DT', 'O']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from nltk import WordNetLemmatizer as wnl\n",
    "\n",
    "inputdata = a2.preprocess(gmbfile)\n",
    "\n",
    "processed_data = list()\n",
    "for tag in inputdata[1:]:\n",
    "    processed_data.append(re.split(\"\\s\", tag)[:-1])\n",
    "    \n",
    "for index, tag in enumerate(processed_data):\n",
    "    processed_data[index][0] = int(float(tag[0]))\n",
    "    processed_data[index][1] = int(float(tag[1]))\n",
    "    processed_data[index][2] = wnl().lemmatize(tag[2].lower())\n",
    "\n",
    "gmbfile.close()\n",
    "\n",
    "processed_data[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Creating instances (7 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do step 2 above.  You will create a collection of Instance objects.  Remember to consider the case where the NE is at the beginning of a sentence or at the end, or close to either (you can create a special start token for that).  You can also start counting from before the `B` end of the NE mention and after the last `I` of the NE mention. That means that the instances should include things before and after the named entity mention, but not the named entity text itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = a2.create_instances(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Class: geo Features: ['of', 'demonstrator', 'have', 'marched', 'through', 'to', 'protest', 'the', 'war', 'in'],\n",
       " Class: geo Features: ['to', 'protest', 'the', 'war', 'in', 'and', 'demand', 'the', 'withdrawal', 'of'],\n",
       " Class: gpe Features: ['and', 'demand', 'the', 'withdrawal', 'of', 'troop', 'from', 'that', 'country', '.'],\n",
       " Class: per Features: ['with', 'such', 'slogan', 'a', '\"\"\"\"', 'number', 'one', 'terrorist', '\"\"\"\"', 'and'],\n",
       " Class: geo Features: ['parliament', 'to', 'a', 'rally', 'in', 'park', '.', '<e1>', '<e2>', '<e3>'],\n",
       " Class: geo Features: ['of', 'the', 'annual', 'conference', 'of', \"'s\", 'ruling', 'labor', 'party', 'in'],\n",
       " Class: org Features: ['conference', 'of', '<ne>', \"'s\", 'ruling', 'party', 'in', '<ne>', 'southern', 'english'],\n",
       " Class: gpe Features: ['<ne>', 'in', 'the', 'southern', 'seaside', 'resort', 'of', '<ne>'],\n",
       " Class: geo Features: ['southern', '<ne>', 'seaside', 'resort', 'of', '.', '<e1>', '<e2>', '<e3>', '<e4>'],\n",
       " Class: gpe Features: ['the', 'party', 'is', 'divided', 'over', \"'s\", 'participation', 'in', 'the', 'iraq'],\n",
       " Class: geo Features: ['<ne>', \"'s\", 'participation', 'in', 'the', 'conflict', 'and', 'the', 'continued', '<ne>'],\n",
       " Class: gpe Features: ['the', 'continued', 'deployment', 'of', '8,500', 'troop', 'in', 'that', 'country', '.'],\n",
       " Class: geo Features: ['<s4>', '<s3>', '<s2>', '<s1>', 'the', 'march', 'came', 'ahead', 'of', 'anti-war'],\n",
       " Class: geo Features: ['in', 'other', 'city', ',', 'including', ',', 'paris', ',', 'and', 'madrid'],\n",
       " Class: geo Features: ['city', ',', 'including', '<ne>', ',', ',', '<ne>', 'madrid', '.', '<e1>'],\n",
       " Class: geo Features: ['<ne>', ',', '<ne>', ',', 'and', '.', '<e1>', '<e2>', '<e3>', '<e4>'],\n",
       " Class: org Features: ['<s4>', '<s3>', '<s2>', '<s1>', 'the', 'atomic', 'energy', '<ne>', 'is', 'to'],\n",
       " Class: geo Features: ['second', 'day', 'of', 'talk', 'in', 'wednesday', 'on', 'how', 'to', 'respond'],\n",
       " Class: tim Features: ['day', 'of', 'talk', 'in', '<ne>', '<ne>', 'how', 'to', 'respond', 'to'],\n",
       " Class: gpe Features: ['on', 'how', 'to', 'respond', 'to', \"'s\", 'resumption', 'of', 'low-level', 'uranium'],\n",
       " Class: gpe Features: ['<s5>', '<s4>', '<s3>', '<s2>', '<s1>', 'this', 'week', 'restarted', 'part', 'of'],\n",
       " Class: geo Features: ['the', 'conversion', 'process', 'at', 'it', 'nuclear', 'plant', '.', '<e1>', '<e2>'],\n",
       " Class: gpe Features: ['<s5>', '<s4>', '<s3>', '<s2>', '<s1>', 'official', 'say', 'they', '<ne>', 'to'],\n",
       " Class: tim Features: ['sensitive', 'part', 'of', 'the', 'plant', ',', 'after', 'an', 'iaea', 'surveillance'],\n",
       " Class: org Features: ['plant', '<ne>', ',', 'after', 'an', 'surveillance', 'system', 'begin', '<ne>', '.'],\n",
       " Class: org Features: ['<s4>', '<s3>', '<s2>', '<s1>', 'the', 'union', ',', 'with', 'u.s.', 'backing'],\n",
       " Class: gpe Features: ['the', '<ne>', ',', 'with', 'backing', ',', '<ne>', 'to'],\n",
       " Class: gpe Features: [',', 'ha', 'threatened', 'to', 'refer', 'to', 'the', 'u.n.', 'security', 'council'],\n",
       " Class: org Features: ['to', 'refer', '<ne>', 'to', 'the', 'security', 'council', '<ne>', 'which', 'could'],\n",
       " Class: gpe Features: ['impose', 'sanction', 'if', 'it', 'find', 'ha', 'violated', 'the', 'nuclear', 'non-proliferation'],\n",
       " Class: art Features: ['find', '<ne>', 'ha', 'violated', 'the', 'non-proliferation', 'treaty', '.', '<e1>', '<e2>'],\n",
       " Class: gpe Features: ['<s5>', '<s4>', '<s3>', '<s2>', '<s1>', \"'s\", 'new', '<ne>', 'ahmadinejad'],\n",
       " Class: per Features: ['<s2>', '<s1>', '<ne>', \"'s\", 'new', 'mahmoud', 'ahmadinejad', '<ne>', 'tuesday', 'that'],\n",
       " Class: tim Features: ['new', '<ne>', 'said', 'that', '<ne>', 'at'],\n",
       " Class: gpe Features: ['said', '<ne>', 'that', 'incentive', '<ne>', 'at'],\n",
       " Class: gpe Features: ['<ne>', 'incentive', 'aimed', 'at', 'persuading', 'to', 'end', 'it', 'nuclear', '<ne>'],\n",
       " Class: gpe Features: ['are', 'an', 'insult', 'to', 'the', 'nation', '.', '<e1>', '<e2>', '<e3>'],\n",
       " Class: gpe Features: ['<s4>', '<s3>', '<s2>', '<s1>', 'two', 'and', 'four', 'nigerian', '<ne>', 'worker'],\n",
       " Class: gpe Features: ['<s1>', 'two', '<ne>', 'and', 'four', 'oil', 'worker', '<ne>', 'kidnapped', 'by'],\n",
       " Class: geo Features: ['raid', 'on', 'a', 'boat', 'in', \"'s\", 'southern', 'oil-rich', 'delta', 'region']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instances[0:40]\n",
    "\n",
    "#instances[0].neclass\n",
    "#instances[1].features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Creating the table and splitting (10 points)\n",
    "\n",
    "Here you're going to write the functions that create a data table with \"document\" vectors representing each instance and split the table into training and testing sets and random with an 80%/20% train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>&lt;ne&gt;</th>\n",
       "      <th>the</th>\n",
       "      <th>,</th>\n",
       "      <th>&lt;s1&gt;</th>\n",
       "      <th>in</th>\n",
       "      <th>&lt;s2&gt;</th>\n",
       "      <th>&lt;s3&gt;</th>\n",
       "      <th>.</th>\n",
       "      <th>&lt;e1&gt;</th>\n",
       "      <th>...</th>\n",
       "      <th>mtetwa</th>\n",
       "      <th>photographer</th>\n",
       "      <th>anbar</th>\n",
       "      <th>u.s</th>\n",
       "      <th>hall</th>\n",
       "      <th>partner</th>\n",
       "      <th>cleveland</th>\n",
       "      <th>ohio</th>\n",
       "      <th>al-arabiya</th>\n",
       "      <th>concluded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>geo</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>geo</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpe</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>per</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>geo</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6917</th>\n",
       "      <td>per</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6918</th>\n",
       "      <td>tim</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6919</th>\n",
       "      <td>geo</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6920</th>\n",
       "      <td>per</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6921</th>\n",
       "      <td>tim</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6922 rows × 3001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     class  <ne>  the    ,  <s1>   in  <s2>  <s3>    .  <e1>  ...  mtetwa  \\\n",
       "0      geo   0.0  1.0  0.0   0.0  1.0   0.0   0.0  0.0   0.0  ...     0.0   \n",
       "1      geo   0.0  2.0  0.0   0.0  1.0   0.0   0.0  0.0   0.0  ...     0.0   \n",
       "2      gpe   0.0  1.0  0.0   0.0  0.0   0.0   0.0  1.0   0.0  ...     0.0   \n",
       "3      per   0.0  0.0  0.0   0.0  0.0   0.0   0.0  0.0   0.0  ...     0.0   \n",
       "4      geo   0.0  0.0  0.0   0.0  1.0   0.0   0.0  1.0   1.0  ...     0.0   \n",
       "...    ...   ...  ...  ...   ...  ...   ...   ...  ...   ...  ...     ...   \n",
       "6917   per   0.0  0.0  0.0   0.0  0.0   0.0   0.0  0.0   0.0  ...     0.0   \n",
       "6918   tim   0.0  1.0  0.0   0.0  1.0   0.0   0.0  0.0   0.0  ...     0.0   \n",
       "6919   geo   0.0  0.0  0.0   0.0  1.0   0.0   0.0  1.0   1.0  ...     0.0   \n",
       "6920   per   0.0  0.0  0.0   0.0  0.0   0.0   0.0  1.0   1.0  ...     0.0   \n",
       "6921   tim   0.0  0.0  0.0   0.0  0.0   0.0   0.0  1.0   1.0  ...     0.0   \n",
       "\n",
       "      photographer  anbar  u.s  hall  partner  cleveland  ohio  al-arabiya  \\\n",
       "0              0.0    0.0  0.0   0.0      0.0        0.0   0.0         0.0   \n",
       "1              0.0    0.0  0.0   0.0      0.0        0.0   0.0         0.0   \n",
       "2              0.0    0.0  0.0   0.0      0.0        0.0   0.0         0.0   \n",
       "3              0.0    0.0  0.0   0.0      0.0        0.0   0.0         0.0   \n",
       "4              0.0    0.0  0.0   0.0      0.0        0.0   0.0         0.0   \n",
       "...            ...    ...  ...   ...      ...        ...   ...         ...   \n",
       "6917           0.0    0.0  0.0   0.0      0.0        0.0   0.0         0.0   \n",
       "6918           0.0    0.0  0.0   0.0      0.0        0.0   0.0         0.0   \n",
       "6919           0.0    0.0  0.0   0.0      0.0        0.0   0.0         0.0   \n",
       "6920           0.0    0.0  0.0   0.0      0.0        0.0   0.0         0.0   \n",
       "6921           0.0    0.0  0.0   0.0      0.0        0.0   0.0         0.0   \n",
       "\n",
       "      concluded  \n",
       "0           0.0  \n",
       "1           0.0  \n",
       "2           0.0  \n",
       "3           0.0  \n",
       "4           0.0  \n",
       "...         ...  \n",
       "6917        0.0  \n",
       "6918        0.0  \n",
       "6919        0.0  \n",
       "6920        0.0  \n",
       "6921        0.0  \n",
       "\n",
       "[6922 rows x 3001 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigdf = a2.create_table(instances)\n",
    "bigdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [2., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
       " 4173    org\n",
       " 1416    gpe\n",
       " 358     tim\n",
       " 2111    org\n",
       " 1154    org\n",
       "        ... \n",
       " 3022    geo\n",
       " 4076    tim\n",
       " 3961    gpe\n",
       " 2395    tim\n",
       " 3514    geo\n",
       " Name: class, Length: 5537, dtype: object,\n",
       " array([[0., 0., 1., ..., 0., 0., 0.],\n",
       "        [2., 0., 1., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
       " 1543    gpe\n",
       " 3508    geo\n",
       " 4696    org\n",
       " 2449    tim\n",
       " 6846    per\n",
       "        ... \n",
       " 3975    org\n",
       " 6262    geo\n",
       " 1074    org\n",
       " 2444    geo\n",
       " 4231    gpe\n",
       " Name: class, Length: 1385, dtype: object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X, train_y, test_X, test_y = a2.ttsplit(bigdf)\n",
    "\n",
    "# X and y mean feature matrix and class respectively.\n",
    "train_X, train_y, test_X, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20008668015024558"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_y) / (len(test_y) + len(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20008668015024558"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_X) / (len(test_X) + len(train_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpe'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(test_y)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - Training the model (0 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part you won't do yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib64/python3.7/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "model = LinearSVC()\n",
    "model.fit(train_X, train_y)\n",
    "train_predictions = model.predict(train_X)\n",
    "test_predictions = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['org', 'gpe', 'tim', ..., 'gpe', 'tim', 'geo'], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4173    org\n",
       "1416    gpe\n",
       "358     tim\n",
       "2111    org\n",
       "1154    org\n",
       "       ... \n",
       "3022    geo\n",
       "4076    tim\n",
       "3961    gpe\n",
       "2395    tim\n",
       "3514    geo\n",
       "Name: class, Length: 5537, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['org', 'geo', 'org', ..., 'tim', 'tim', 'gpe'], dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1543    gpe\n",
       "3508    geo\n",
       "4696    org\n",
       "2449    tim\n",
       "6846    per\n",
       "       ... \n",
       "3975    org\n",
       "6262    geo\n",
       "1074    org\n",
       "2444    geo\n",
       "4231    gpe\n",
       "Name: class, Length: 1385, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 - Evaluation (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate for yourself what a \"confusion matrix\".  Then implement a function that takes the data and produces a confusion matrix in any readable form that allows us to compare the performance of the model by class.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     gpe  org  geo  tim  per  eve  nat  art\n",
      "gpe   88   36   69   25   30    0    0    1\n",
      "org   39  103   69   28   26    0    0    2\n",
      "geo   35   65  229   52   20    0    0    0\n",
      "tim   27   29   71   92   20    1    0    1\n",
      "per   28   29   27   14  110    0    0    2\n",
      "eve    0    3    1    2    1    2    0    0\n",
      "nat    1    0    2    0    0    0    1    0\n",
      "art    1    1    1    1    0    0    0    0\n",
      "\n",
      "Some info: \n",
      "{\n",
      "    'art': {\n",
      "        'correct_percentage': 0.9927797833935018,\n",
      "        'fn': 4,\n",
      "        'fp': 6,\n",
      "        'tn': 1375,\n",
      "        'tp': 0,\n",
      "        'wrong_percentage': 0.007220216606498195,\n",
      "    },\n",
      "    'eve': {\n",
      "        'correct_percentage': 0.9942238267148015,\n",
      "        'fn': 7,\n",
      "        'fp': 1,\n",
      "        'tn': 1375,\n",
      "        'tp': 2,\n",
      "        'wrong_percentage': 0.005776173285198556,\n",
      "    },\n",
      "    'geo': {\n",
      "        'correct_percentage': 0.7025270758122744,\n",
      "        'fn': 172,\n",
      "        'fp': 240,\n",
      "        'tn': 744,\n",
      "        'tp': 229,\n",
      "        'wrong_percentage': 0.29747292418772564,\n",
      "    },\n",
      "    'gpe': {\n",
      "        'correct_percentage': 0.7891696750902527,\n",
      "        'fn': 161,\n",
      "        'fp': 131,\n",
      "        'tn': 1005,\n",
      "        'tp': 88,\n",
      "        'wrong_percentage': 0.2108303249097473,\n",
      "    },\n",
      "    'nat': {\n",
      "        'correct_percentage': 0.9978339350180505,\n",
      "        'fn': 3,\n",
      "        'fp': 0,\n",
      "        'tn': 1381,\n",
      "        'tp': 1,\n",
      "        'wrong_percentage': 0.0021660649819494585,\n",
      "    },\n",
      "    'org': {\n",
      "        'correct_percentage': 0.763898916967509,\n",
      "        'fn': 164,\n",
      "        'fp': 163,\n",
      "        'tn': 955,\n",
      "        'tp': 103,\n",
      "        'wrong_percentage': 0.23610108303249097,\n",
      "    },\n",
      "    'per': {\n",
      "        'correct_percentage': 0.8577617328519855,\n",
      "        'fn': 100,\n",
      "        'fp': 97,\n",
      "        'tn': 1078,\n",
      "        'tp': 110,\n",
      "        'wrong_percentage': 0.14223826714801444,\n",
      "    },\n",
      "    'tim': {\n",
      "        'correct_percentage': 0.8043321299638989,\n",
      "        'fn': 149,\n",
      "        'fp': 122,\n",
      "        'tn': 1022,\n",
      "        'tp': 92,\n",
      "        'wrong_percentage': 0.1956678700361011,\n",
      "    },\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprintpp import pprint\n",
    "\n",
    "info = a2.info_associated_w_cf(test_y, test_predictions)\n",
    "confusion = a2.confusion_matrix(test_y, test_predictions)\n",
    "\n",
    "print(confusion)\n",
    "print(\"\\nSome info: \");pprint(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     org  gpe  tim   geo  eve  per  art  nat\n",
      "org  841   34   13    59    0   23    0    0\n",
      "gpe   18  872   11    62    0   18    0    0\n",
      "tim    8   11  822    62    0   16    0    0\n",
      "geo   47   44   45  1519    0   14    0    0\n",
      "eve    1    0    1     2   32    0    0    0\n",
      "per   16   20   10    22    0  829    0    0\n",
      "art    2    0    2     1    0    0   44    0\n",
      "nat    0    0    0     1    0    0    0   15\n",
      "\n",
      "Some info: \n",
      "{\n",
      "    'art': {\n",
      "        'correct_percentage': 0.9990969839263139,\n",
      "        'fn': 5,\n",
      "        'fp': 0,\n",
      "        'tn': 5488,\n",
      "        'tp': 44,\n",
      "        'wrong_percentage': 0.0009030160736861117,\n",
      "    },\n",
      "    'eve': {\n",
      "        'correct_percentage': 0.9992775871410511,\n",
      "        'fn': 4,\n",
      "        'fp': 0,\n",
      "        'tn': 5501,\n",
      "        'tp': 32,\n",
      "        'wrong_percentage': 0.0007224128589488893,\n",
      "    },\n",
      "    'geo': {\n",
      "        'correct_percentage': 0.9351634459093372,\n",
      "        'fn': 150,\n",
      "        'fp': 209,\n",
      "        'tn': 3659,\n",
      "        'tp': 1519,\n",
      "        'wrong_percentage': 0.06483655409066282,\n",
      "    },\n",
      "    'gpe': {\n",
      "        'correct_percentage': 0.9606284991872855,\n",
      "        'fn': 109,\n",
      "        'fp': 109,\n",
      "        'tn': 4447,\n",
      "        'tp': 872,\n",
      "        'wrong_percentage': 0.03937150081271447,\n",
      "    },\n",
      "    'nat': {\n",
      "        'correct_percentage': 0.9998193967852628,\n",
      "        'fn': 1,\n",
      "        'fp': 0,\n",
      "        'tn': 5521,\n",
      "        'tp': 15,\n",
      "        'wrong_percentage': 0.00018060321473722233,\n",
      "    },\n",
      "    'org': {\n",
      "        'correct_percentage': 0.9600866895430739,\n",
      "        'fn': 129,\n",
      "        'fp': 92,\n",
      "        'tn': 4475,\n",
      "        'tp': 841,\n",
      "        'wrong_percentage': 0.039913310456926136,\n",
      "    },\n",
      "    'per': {\n",
      "        'correct_percentage': 0.9748961531515261,\n",
      "        'fn': 68,\n",
      "        'fp': 71,\n",
      "        'tn': 4569,\n",
      "        'tp': 829,\n",
      "        'wrong_percentage': 0.0251038468484739,\n",
      "    },\n",
      "    'tim': {\n",
      "        'correct_percentage': 0.9676720245620372,\n",
      "        'fn': 97,\n",
      "        'fp': 82,\n",
      "        'tn': 4536,\n",
      "        'tp': 822,\n",
      "        'wrong_percentage': 0.03232797543796279,\n",
      "    },\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "info = a2.info_associated_w_cf(train_y, train_predictions)\n",
    "confusion = a2.confusion_matrix(train_y, train_predictions)\n",
    "\n",
    "print(confusion)\n",
    "print(\"\\nSome info: \");pprint(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the matrix and describe your observations in README.md.  In particular, what do you notice about the predictions on the training data compared to those on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Part A - Error analysis (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the weakest-performing classes in the confusion matrix (or any, if they all perform poorly to the same extent).  Find some examples in the test data on which the classifier classified incorrectly for those classes.  What do you think is the reason why those are hard?  Consider linguistic factors and statistical factors, if applicable.  Write your answer in README.md."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Part B - Expanding the feature space (7 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the entire process above, but incorporate part-of-speech tag information into the feature vectors.  It's your choice as to how to do this, but document it in README.md.  Your new process should run from the single call below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib64/python3.7/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix with word tags mixed with P.O.S tags (tested against test set):\n",
      "     geo  per  gpe  org  tim  eve  art  nat\n",
      "geo  238   26   54   52   56    0    1    0\n",
      "per   36  122   25   22   23    0    0    0\n",
      "gpe   44   21  110   40   24    0    0    0\n",
      "org   55   18   40   94   20    0    0    0\n",
      "tim   58   17   29   25  107    1    2    0\n",
      "eve    5    1    0    1    3    1    0    0\n",
      "art    1    1    0    2    2    0    1    0\n",
      "nat    1    1    0    2    3    0    0    0\n",
      "\n",
      "Some additional data: \n",
      "{\n",
      "    'art': {\n",
      "        'correct_percentage': 0.9935018050541516,\n",
      "        'fn': 6,\n",
      "        'fp': 3,\n",
      "        'tn': 1375,\n",
      "        'tp': 1,\n",
      "        'wrong_percentage': 0.006498194945848376,\n",
      "    },\n",
      "    'eve': {\n",
      "        'correct_percentage': 0.992057761732852,\n",
      "        'fn': 10,\n",
      "        'fp': 1,\n",
      "        'tn': 1373,\n",
      "        'tp': 1,\n",
      "        'wrong_percentage': 0.007942238267148015,\n",
      "    },\n",
      "    'geo': {\n",
      "        'correct_percentage': 0.7191335740072202,\n",
      "        'fn': 189,\n",
      "        'fp': 200,\n",
      "        'tn': 758,\n",
      "        'tp': 238,\n",
      "        'wrong_percentage': 0.2808664259927798,\n",
      "    },\n",
      "    'gpe': {\n",
      "        'correct_percentage': 0.8,\n",
      "        'fn': 129,\n",
      "        'fp': 148,\n",
      "        'tn': 998,\n",
      "        'tp': 110,\n",
      "        'wrong_percentage': 0.2,\n",
      "    },\n",
      "    'nat': {\n",
      "        'correct_percentage': 0.9949458483754513,\n",
      "        'fn': 7,\n",
      "        'fp': 0,\n",
      "        'tn': 1378,\n",
      "        'tp': 0,\n",
      "        'wrong_percentage': 0.005054151624548736,\n",
      "    },\n",
      "    'org': {\n",
      "        'correct_percentage': 0.8,\n",
      "        'fn': 133,\n",
      "        'fp': 144,\n",
      "        'tn': 1014,\n",
      "        'tp': 94,\n",
      "        'wrong_percentage': 0.2,\n",
      "    },\n",
      "    'per': {\n",
      "        'correct_percentage': 0.8620938628158845,\n",
      "        'fn': 106,\n",
      "        'fp': 85,\n",
      "        'tn': 1072,\n",
      "        'tp': 122,\n",
      "        'wrong_percentage': 0.1379061371841155,\n",
      "    },\n",
      "    'tim': {\n",
      "        'correct_percentage': 0.8101083032490974,\n",
      "        'fn': 132,\n",
      "        'fp': 131,\n",
      "        'tn': 1015,\n",
      "        'tp': 107,\n",
      "        'wrong_percentage': 0.18989169675090253,\n",
      "    },\n",
      "}\n",
      "Confusion matrix with word tags mixed with P.O.S tags (tested against training set):\n",
      "      geo  tim  org  per  gpe  eve  art  nat\n",
      "geo  1509   36   35   17   46    0    0    0\n",
      "tim    56  843    7    5   10    0    0    0\n",
      "org    56   11  894   18   31    0    0    0\n",
      "per    12    5    7  839   16    0    0    0\n",
      "gpe    55   11   25   13  887    0    0    0\n",
      "eve     1    1    0    0    0   32    0    0\n",
      "art     0    1    0    1    0    0   44    0\n",
      "nat     0    0    0    0    0    0    0   13\n",
      "\n",
      "Some additional data: \n",
      "{\n",
      "    'art': {\n",
      "        'correct_percentage': 0.9996387935705255,\n",
      "        'fn': 2,\n",
      "        'fp': 0,\n",
      "        'tn': 5491,\n",
      "        'tp': 44,\n",
      "        'wrong_percentage': 0.00036120642947444465,\n",
      "    },\n",
      "    'eve': {\n",
      "        'correct_percentage': 0.9996387935705255,\n",
      "        'fn': 2,\n",
      "        'fp': 0,\n",
      "        'tn': 5503,\n",
      "        'tp': 32,\n",
      "        'wrong_percentage': 0.00036120642947444465,\n",
      "    },\n",
      "    'geo': {\n",
      "        'correct_percentage': 0.9432905905725122,\n",
      "        'fn': 134,\n",
      "        'fp': 180,\n",
      "        'tn': 3714,\n",
      "        'tp': 1509,\n",
      "        'wrong_percentage': 0.05670940942748781,\n",
      "    },\n",
      "    'gpe': {\n",
      "        'correct_percentage': 0.962615134549395,\n",
      "        'fn': 104,\n",
      "        'fp': 103,\n",
      "        'tn': 4443,\n",
      "        'tp': 887,\n",
      "        'wrong_percentage': 0.03738486545060502,\n",
      "    },\n",
      "    'nat': {\n",
      "        'correct_percentage': 1.0,\n",
      "        'fn': 0,\n",
      "        'fp': 0,\n",
      "        'tn': 5524,\n",
      "        'tp': 13,\n",
      "        'wrong_percentage': 0.0,\n",
      "    },\n",
      "    'org': {\n",
      "        'correct_percentage': 0.9656853891999277,\n",
      "        'fn': 116,\n",
      "        'fp': 74,\n",
      "        'tn': 4453,\n",
      "        'tp': 894,\n",
      "        'wrong_percentage': 0.03431461080007224,\n",
      "    },\n",
      "    'per': {\n",
      "        'correct_percentage': 0.9830232978147011,\n",
      "        'fn': 40,\n",
      "        'fp': 54,\n",
      "        'tn': 4604,\n",
      "        'tp': 839,\n",
      "        'wrong_percentage': 0.016976702185298898,\n",
      "    },\n",
      "    'tim': {\n",
      "        'correct_percentage': 0.9741737402925772,\n",
      "        'fn': 78,\n",
      "        'fp': 65,\n",
      "        'tn': 4551,\n",
      "        'tp': 843,\n",
      "        'wrong_percentage': 0.02582625970742279,\n",
      "    },\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib64/python3.7/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix with word tags only (tested against test set):\n",
      "     tim  gpe  org  geo  per  art  nat  eve\n",
      "tim   86   29   22   67   25    1    0    0\n",
      "gpe   14  111   39   64   32    1    0    0\n",
      "org   23   42   76   54   39    1    0    1\n",
      "geo   43   54   53  214   33    0    1    0\n",
      "per   17   40   30   38  105    0    0    0\n",
      "art    1    5    4    2    1    2    0    0\n",
      "nat    0    1    1    0    1    0    0    0\n",
      "eve    1    1    1    4    2    0    0    3\n",
      "\n",
      "Some additional data: \n",
      "{\n",
      "    'art': {\n",
      "        'correct_percentage': 0.9884476534296028,\n",
      "        'fn': 13,\n",
      "        'fp': 3,\n",
      "        'tn': 1367,\n",
      "        'tp': 2,\n",
      "        'wrong_percentage': 0.011552346570397111,\n",
      "    },\n",
      "    'eve': {\n",
      "        'correct_percentage': 0.9927797833935018,\n",
      "        'fn': 9,\n",
      "        'fp': 1,\n",
      "        'tn': 1372,\n",
      "        'tp': 3,\n",
      "        'wrong_percentage': 0.007220216606498195,\n",
      "    },\n",
      "    'geo': {\n",
      "        'correct_percentage': 0.7018050541516245,\n",
      "        'fn': 184,\n",
      "        'fp': 229,\n",
      "        'tn': 758,\n",
      "        'tp': 214,\n",
      "        'wrong_percentage': 0.29819494584837547,\n",
      "    },\n",
      "    'gpe': {\n",
      "        'correct_percentage': 0.7675090252707581,\n",
      "        'fn': 150,\n",
      "        'fp': 172,\n",
      "        'tn': 952,\n",
      "        'tp': 111,\n",
      "        'wrong_percentage': 0.23249097472924188,\n",
      "    },\n",
      "    'nat': {\n",
      "        'correct_percentage': 0.9971119133574007,\n",
      "        'fn': 3,\n",
      "        'fp': 1,\n",
      "        'tn': 1381,\n",
      "        'tp': 0,\n",
      "        'wrong_percentage': 0.002888086642599278,\n",
      "    },\n",
      "    'org': {\n",
      "        'correct_percentage': 0.776173285198556,\n",
      "        'fn': 160,\n",
      "        'fp': 150,\n",
      "        'tn': 999,\n",
      "        'tp': 76,\n",
      "        'wrong_percentage': 0.22382671480144403,\n",
      "    },\n",
      "    'per': {\n",
      "        'correct_percentage': 0.8137184115523466,\n",
      "        'fn': 125,\n",
      "        'fp': 133,\n",
      "        'tn': 1022,\n",
      "        'tp': 105,\n",
      "        'wrong_percentage': 0.18628158844765344,\n",
      "    },\n",
      "    'tim': {\n",
      "        'correct_percentage': 0.8245487364620938,\n",
      "        'fn': 144,\n",
      "        'fp': 99,\n",
      "        'tn': 1056,\n",
      "        'tp': 86,\n",
      "        'wrong_percentage': 0.17545126353790613,\n",
      "    },\n",
      "}\n",
      "Confusion matrix with word tags only (tested against training set):\n",
      "     gpe   geo  org  per  tim  art  nat  eve\n",
      "gpe  853    60   25   19   12    0    0    0\n",
      "geo   47  1512   44   23   46    0    0    0\n",
      "org   33    59  872   20   17    0    0    0\n",
      "per   19    22    9  820    7    0    0    0\n",
      "tim   11    65   12   14  828    0    0    0\n",
      "art    0     0    2    0    0   36    0    0\n",
      "nat    0     0    0    0    0    0   17    0\n",
      "eve    0     2    0    0    1    0    0   30\n",
      "\n",
      "Some additional data: \n",
      "{\n",
      "    'art': {\n",
      "        'correct_percentage': 0.9996387935705255,\n",
      "        'fn': 2,\n",
      "        'fp': 0,\n",
      "        'tn': 5499,\n",
      "        'tp': 36,\n",
      "        'wrong_percentage': 0.00036120642947444465,\n",
      "    },\n",
      "    'eve': {\n",
      "        'correct_percentage': 0.9994581903557883,\n",
      "        'fn': 3,\n",
      "        'fp': 0,\n",
      "        'tn': 5504,\n",
      "        'tp': 30,\n",
      "        'wrong_percentage': 0.000541809644211667,\n",
      "    },\n",
      "    'geo': {\n",
      "        'correct_percentage': 0.9335380169767021,\n",
      "        'fn': 160,\n",
      "        'fp': 208,\n",
      "        'tn': 3657,\n",
      "        'tp': 1512,\n",
      "        'wrong_percentage': 0.06646198302329781,\n",
      "    },\n",
      "    'gpe': {\n",
      "        'correct_percentage': 0.9591836734693877,\n",
      "        'fn': 116,\n",
      "        'fp': 110,\n",
      "        'tn': 4458,\n",
      "        'tp': 853,\n",
      "        'wrong_percentage': 0.04081632653061224,\n",
      "    },\n",
      "    'nat': {\n",
      "        'correct_percentage': 1.0,\n",
      "        'fn': 0,\n",
      "        'fp': 0,\n",
      "        'tn': 5520,\n",
      "        'tp': 17,\n",
      "        'wrong_percentage': 0.0,\n",
      "    },\n",
      "    'org': {\n",
      "        'correct_percentage': 0.9600866895430739,\n",
      "        'fn': 129,\n",
      "        'fp': 92,\n",
      "        'tn': 4444,\n",
      "        'tp': 872,\n",
      "        'wrong_percentage': 0.039913310456926136,\n",
      "    },\n",
      "    'per': {\n",
      "        'correct_percentage': 0.9759797724399494,\n",
      "        'fn': 57,\n",
      "        'fp': 76,\n",
      "        'tn': 4584,\n",
      "        'tp': 820,\n",
      "        'wrong_percentage': 0.02402022756005057,\n",
      "    },\n",
      "    'tim': {\n",
      "        'correct_percentage': 0.9665884052736139,\n",
      "        'fn': 102,\n",
      "        'fp': 83,\n",
      "        'tn': 4524,\n",
      "        'tp': 828,\n",
      "        'wrong_percentage': 0.03341159472638613,\n",
      "    },\n",
      "}\n",
      "Confusion matrix with with P.O.S tags only (tested against test set):\n",
      "     org  per  geo  tim  gpe  art  nat  eve\n",
      "org   80   41   87   24   16    0    0    0\n",
      "per   25  104   58   19   18    0    0    0\n",
      "geo   33   31  262   38   16    0    0    0\n",
      "tim   23   27  112   69    5    0    0    0\n",
      "gpe   55   56  110   15   33    0    0    0\n",
      "art    2    1    7    0    0    0    0    0\n",
      "nat    0    0    4    1    0    0    0    0\n",
      "eve    2    1    7    3    0    0    0    0\n",
      "\n",
      "Some additional data: \n",
      "{\n",
      "    'art': {\n",
      "        'correct_percentage': 0.9927797833935018,\n",
      "        'fn': 10,\n",
      "        'fp': 0,\n",
      "        'tn': 1375,\n",
      "        'tp': 0,\n",
      "        'wrong_percentage': 0.007220216606498195,\n",
      "    },\n",
      "    'eve': {\n",
      "        'correct_percentage': 0.9906137184115523,\n",
      "        'fn': 13,\n",
      "        'fp': 0,\n",
      "        'tn': 1372,\n",
      "        'tp': 0,\n",
      "        'wrong_percentage': 0.009386281588447653,\n",
      "    },\n",
      "    'geo': {\n",
      "        'correct_percentage': 0.6368231046931407,\n",
      "        'fn': 118,\n",
      "        'fp': 385,\n",
      "        'tn': 620,\n",
      "        'tp': 262,\n",
      "        'wrong_percentage': 0.3631768953068592,\n",
      "    },\n",
      "    'gpe': {\n",
      "        'correct_percentage': 0.7898916967509025,\n",
      "        'fn': 236,\n",
      "        'fp': 55,\n",
      "        'tn': 1061,\n",
      "        'tp': 33,\n",
      "        'wrong_percentage': 0.21010830324909746,\n",
      "    },\n",
      "    'nat': {\n",
      "        'correct_percentage': 0.9963898916967509,\n",
      "        'fn': 5,\n",
      "        'fp': 0,\n",
      "        'tn': 1380,\n",
      "        'tp': 0,\n",
      "        'wrong_percentage': 0.0036101083032490976,\n",
      "    },\n",
      "    'org': {\n",
      "        'correct_percentage': 0.7776173285198555,\n",
      "        'fn': 168,\n",
      "        'fp': 140,\n",
      "        'tn': 997,\n",
      "        'tp': 80,\n",
      "        'wrong_percentage': 0.2223826714801444,\n",
      "    },\n",
      "    'per': {\n",
      "        'correct_percentage': 0.8,\n",
      "        'fn': 120,\n",
      "        'fp': 157,\n",
      "        'tn': 1004,\n",
      "        'tp': 104,\n",
      "        'wrong_percentage': 0.2,\n",
      "    },\n",
      "    'tim': {\n",
      "        'correct_percentage': 0.8072202166064982,\n",
      "        'fn': 167,\n",
      "        'fp': 100,\n",
      "        'tn': 1049,\n",
      "        'tp': 69,\n",
      "        'wrong_percentage': 0.1927797833935018,\n",
      "    },\n",
      "}\n",
      "Confusion matrix with with P.O.S tags only (tested against training set):\n",
      "      geo  tim  gpe  org  per  nat  art  eve\n",
      "geo  1184  117   64  167  158    0    0    0\n",
      "tim   480  249   37   82   76    0    0    0\n",
      "gpe   387   83  112  180  199    0    0    0\n",
      "org   368   81   70  328  142    0    0    0\n",
      "per   295   41   54   71  422    0    0    0\n",
      "nat     8    3    1    2    1    0    0    0\n",
      "art    20    4    3   11    5    0    0    0\n",
      "eve    19    4    3    2    4    0    0    0\n",
      "\n",
      "Some additional data: \n",
      "{\n",
      "    'art': {\n",
      "        'correct_percentage': 0.9922340617662995,\n",
      "        'fn': 43,\n",
      "        'fp': 0,\n",
      "        'tn': 5494,\n",
      "        'tp': 0,\n",
      "        'wrong_percentage': 0.00776593823370056,\n",
      "    },\n",
      "    'eve': {\n",
      "        'correct_percentage': 0.9942206971284089,\n",
      "        'fn': 32,\n",
      "        'fp': 0,\n",
      "        'tn': 5505,\n",
      "        'tp': 0,\n",
      "        'wrong_percentage': 0.0057793028715911144,\n",
      "    },\n",
      "    'geo': {\n",
      "        'correct_percentage': 0.6238035037023659,\n",
      "        'fn': 506,\n",
      "        'fp': 1577,\n",
      "        'tn': 2270,\n",
      "        'tp': 1184,\n",
      "        'wrong_percentage': 0.3761964962976341,\n",
      "    },\n",
      "    'gpe': {\n",
      "        'correct_percentage': 0.8047679248690627,\n",
      "        'fn': 849,\n",
      "        'fp': 232,\n",
      "        'tn': 4344,\n",
      "        'tp': 112,\n",
      "        'wrong_percentage': 0.19523207513093732,\n",
      "    },\n",
      "    'nat': {\n",
      "        'correct_percentage': 0.9972909517789417,\n",
      "        'fn': 15,\n",
      "        'fp': 0,\n",
      "        'tn': 5522,\n",
      "        'tp': 0,\n",
      "        'wrong_percentage': 0.0027090482210583348,\n",
      "    },\n",
      "    'org': {\n",
      "        'correct_percentage': 0.7876106194690266,\n",
      "        'fn': 661,\n",
      "        'fp': 515,\n",
      "        'tn': 4033,\n",
      "        'tp': 328,\n",
      "        'wrong_percentage': 0.21238938053097345,\n",
      "    },\n",
      "    'per': {\n",
      "        'correct_percentage': 0.8110890373848655,\n",
      "        'fn': 461,\n",
      "        'fp': 585,\n",
      "        'tn': 4069,\n",
      "        'tp': 422,\n",
      "        'wrong_percentage': 0.18891096261513454,\n",
      "    },\n",
      "    'tim': {\n",
      "        'correct_percentage': 0.8179519595448799,\n",
      "        'fn': 675,\n",
      "        'fp': 333,\n",
      "        'tn': 4280,\n",
      "        'tp': 249,\n",
      "        'wrong_percentage': 0.1820480404551201,\n",
      "    },\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "###### a2.bonusb('/scratch/lt2222-v21-resources/GMB_dataset.txt')\n",
    "import a2\n",
    "from sklearn.svm import LinearSVC\n",
    "import re\n",
    "from nltk import WordNetLemmatizer as wnl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprintpp import pprint\n",
    "\n",
    "def perform_train_test(instance_list):\n",
    "    bigdf = a2.create_table(instance_list)\n",
    "    train_X, train_y, test_X, test_y = a2.ttsplit(bigdf)\n",
    "    \n",
    "    model = LinearSVC(max_iter=2000)\n",
    "    model.fit(train_X, train_y)\n",
    "    train_pred = model.predict(train_X)\n",
    "    test_pred = model.predict(test_X)\n",
    "    \n",
    "    info_train = a2.info_associated_w_cf(train_y, train_pred)\n",
    "    confusion_train = a2.confusion_matrix(train_y, train_pred)\n",
    "        \n",
    "    info_test = a2.info_associated_w_cf(test_y, test_pred)\n",
    "    confusion_test = a2.confusion_matrix(test_y, test_pred)\n",
    "        \n",
    "    return info_train, confusion_train, info_test, confusion_test\n",
    "\n",
    "def bonusb(filename, process_pos_words_separately = False):\n",
    "    gmbfile = open(filename, \"r\")\n",
    "    inputdata = a2.preprocess(gmbfile)\n",
    "\n",
    "    processed_data = list()\n",
    "    for tag in inputdata[1:]:\n",
    "        processed_data.append(re.split(\"\\s\", tag)[:-1])\n",
    "    \n",
    "    for index, tag in enumerate(processed_data):\n",
    "        processed_data[index][0] = int(float(tag[0]))\n",
    "        processed_data[index][1] = int(float(tag[1]))\n",
    "        processed_data[index][2] = wnl().lemmatize(tag[2].lower())\n",
    "\n",
    "    gmbfile.close()\n",
    "\n",
    "    #I modified create_instances() to simply append the pos tags alongside each word in the features. \n",
    "    #These perhaps should be 'split up', since pos-tags are not the same data as the actual (neighbouring) words. \n",
    "    #We could perhaps do Linear SVC on both sets separately (once on pos-tags only, once on the\n",
    "    #    words themselves only, and combine them somehow perhaps using LinearSVC a second time). \n",
    "    #This would mimic a multi-layered classifier. Though it may perform better, the LinearSVC should do a\n",
    "    #    decent job on it's own as it should compensate for the relative contribution of tags (to a degree).\n",
    "    \n",
    "    if process_pos_words_separately:\n",
    "        instances, instances_pos = a2.create_instances(processed_data, pos_include = True, split_pos_vs_words = process_pos_words_separately)\n",
    "    \n",
    "        info_train, confusion_train, info_test, confusion_test = perform_train_test(instances)\n",
    "        info_train_pos, confusion_train_pos, info_test_pos, confusion_test_pos = perform_train_test(instances_pos)\n",
    "        \n",
    "        print(\"Confusion matrix with word tags only (tested against test set):\")\n",
    "        print(confusion_test)\n",
    "        print(\"\\nSome additional data: \");pprint(info_test)\n",
    "    \n",
    "        print(\"Confusion matrix with word tags only (tested against training set):\")\n",
    "        print(confusion_train)\n",
    "        print(\"\\nSome additional data: \");pprint(info_train)\n",
    "        \n",
    "        print(\"Confusion matrix with with P.O.S tags only (tested against test set):\")\n",
    "        print(confusion_test_pos)\n",
    "        print(\"\\nSome additional data: \");pprint(info_test_pos)\n",
    "    \n",
    "        print(\"Confusion matrix with with P.O.S tags only (tested against training set):\")\n",
    "        print(confusion_train_pos)\n",
    "        print(\"\\nSome additional data: \");pprint(info_train_pos)\n",
    "        \n",
    "    else:\n",
    "        instances_with_pos = a2.create_instances(processed_data, pos_include = True, split_pos_vs_words = process_pos_words_separately)\n",
    "    \n",
    "        info_train, confusion_train, info_test, confusion_test = perform_train_test(instances_with_pos)\n",
    "\n",
    "        print(\"Confusion matrix with word tags mixed with P.O.S tags (tested against test set):\")\n",
    "        print(confusion_test)\n",
    "        print(\"\\nSome additional data: \");pprint(info_test)\n",
    "    \n",
    "        print(\"Confusion matrix with word tags mixed with P.O.S tags (tested against training set):\")\n",
    "        print(confusion_train)\n",
    "        print(\"\\nSome additional data: \");pprint(info_train)\n",
    "    \n",
    "#Mix pos tags with word tags\n",
    "bonusb('/scratch/lt2222-v21-resources/GMB_dataset.txt', process_pos_words_separately = False)\n",
    "\n",
    "#Separate pos and word tags and evaluate separately\n",
    "bonusb('/scratch/lt2222-v21-resources/GMB_dataset.txt', process_pos_words_separately = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
